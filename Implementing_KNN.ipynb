{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auNQ9Q1SR3Um",
        "outputId": "06bf33c6-a3b9-4ffe-df19-bfa0348be1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 73.33%\n",
            "Accuracy with Manhattan distance: 100.00%\n",
            "Accuracy with Minkowski distance: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class KNN:\n",
        "  def __init__(self,k=3,distanceMetric='euclidean'):\n",
        "    self.k=3\n",
        "    self.distanceMetric=distanceMetric\n",
        "\n",
        "  # x1,x2=vector in feature space\n",
        "  def _euclideanDist(self,x1,x2):\n",
        "    return np.sqrt(np.sum(x1-x2)**2)\n",
        "\n",
        "  def _manHattanDist(self,x1,x2):\n",
        "    return np.sum(np.abs(x1-x2))\n",
        "\n",
        "  def _minkowskiDist(self,x1,x2):\n",
        "    return np.sum(np.abs(x1-x2)**self.k)**1/self.k\n",
        "\n",
        "  def fit(self,X,y):\n",
        "    # X : array-like Training data\n",
        "    # y : array-like Target values\n",
        "    self.x_train=X\n",
        "    self.y_train=y\n",
        "\n",
        "  def predict(self,X):\n",
        "    predicted=[self._predict(x) for x in X]\n",
        "    return np.array(predicted)\n",
        "\n",
        "  def _predict(self,x):\n",
        "    if self.distanceMetric=='euclidean':\n",
        "      distance_vector = [self._euclideanDist(x, x_train) for x_train in self.x_train]\n",
        "    elif self.distanceMetric=='manhattan':\n",
        "      distance_vector = [self._manHattanDist(x, x_train) for x_train in self.x_train]\n",
        "    elif self.distanceMetric=='minkowski':\n",
        "      distance_vector = [self._minkowskiDist(x, x_train) for x_train in self.x_train]\n",
        "    else:\n",
        "      raise ValueError(f\"No metric named {self.distanceMetric}\\n Choose from:euclidean manhattan or minkowski\")\n",
        "   # Sort by distance and return indices of the first k neighbors\n",
        "    k_indices = np.argsort(distance_vector)[:self.k]\n",
        "    # Extract the labels of the k nearest neighbor training samples\n",
        "    k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "    # return the most common class label\n",
        "    most_common = Counter(k_nearest_labels).most_common(1)\n",
        "    return most_common[0][0]\n",
        "\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Create KNN classifier\n",
        "knn = KNN()\n",
        "\n",
        "# Fit the classifier to the data\n",
        "knn.fit(X_train_std, y_train)\n",
        "\n",
        "# Predict the labels of the test set using Euclidean distance\n",
        "y_pred_euclidean = knn.predict(X_test_std)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(f'Accuracy with Euclidean distance: {accuracy_euclidean:.2%}')\n",
        "\n",
        "# Now let's test with Manhattan distance\n",
        "knn_manhattan = KNN(distanceMetric='manhattan')\n",
        "knn_manhattan.fit(X_train_std, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_std)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(f'Accuracy with Manhattan distance: {accuracy_manhattan:.2%}')\n",
        "\n",
        "# Finally, test with Minkowski distance (k=3)\n",
        "knn_minkowski = KNN(distanceMetric='minkowski')\n",
        "knn_minkowski.fit(X_train_std, y_train)\n",
        "y_pred_minkowski = knn_minkowski.predict(X_test_std)\n",
        "accuracy_minkowski = accuracy_score(y_test, y_pred_minkowski)\n",
        "print(f'Accuracy with Minkowski distance: {accuracy_minkowski:.2%}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to choose the right Distance Metric?\n",
        "1. What’s our data like?\n",
        "  Continuous vs. Categorical: If our data is all about numbers and measurements (continuous data), Euclidean distance is our go-to, because it measures straight lines between points. For data that’s more about categories (like types of fruit, where “apple” and “orange” aren’t on a scale), Hamming distance, which checks if features match, makes more sense.\n",
        "\n",
        "\n",
        "2. How big is our data?\n",
        "\n",
        "  When our dataset is really wide (lots of features), traditional ideas of closeness get wonky, and everything starts to seem far apart. Here, reducing dimensions or picking metrics suited for the big stage, like cosine similarity for text, can keep things in perspective.\n",
        "\n",
        "\n",
        "3. How is our data spread out?\n",
        "\n",
        "  The way our data is distributed matters. If outliers are a big deal in our dataset, Manhattan distance might be your ally since it doesn’t get as shaken up by extreme values compared to Euclidean distance.\n",
        "\n",
        "\n",
        "4. Need for speed?\n",
        "  Some distance metrics are computationally more intensive than others. Metrics like Manhattan distance can be computationally more efficient than Euclidean distance in certain implementations since it lacks the square root operation."
      ],
      "metadata": {
        "id": "Mj0JNmFnnIzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using ScikitLearn's Inbuilt library\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Create KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Fit the classifier to the data\n",
        "knn.fit(X_train_std, y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = knn.predict(X_test_std)\n",
        "\n",
        "# Print the accuracy of the classifier\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2%}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFKq1ZIDmHfj",
        "outputId": "2c0b0bd2-f3e1-461f-bd54-62d9e43e0f6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}